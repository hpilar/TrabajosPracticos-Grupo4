{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_m3CnbNMV2Cp"
   },
   "source": [
    "# Trabajo Práctico 3 - Regularización aplicada a la EPH \n",
    "\n",
    "## Gil Deza, Hüppi Lo Prete, Walker "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "BoSQOK1iV2Cs"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures \n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utilizamos el código de la clase tutorial para crear un botón para ocultar el código \n",
    "#Fuente: https://stackoverflow.com/questions/27934885/how-to-hide-code-from-cells-in-ipython-notebook-visualized-with-nbviewer\n",
    "\n",
    "import ipywidgets as widgets\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "javascript_functions = {False: \"hide()\", True: \"show()\"}\n",
    "button_descriptions  = {False: \"Show code\", True: \"Hide code\"}\n",
    "\n",
    "\n",
    "def toggle_code(state):\n",
    "\n",
    "    \"\"\"\n",
    "    Toggles the JavaScript show()/hide() function on the div.input element.\n",
    "    \"\"\"\n",
    "\n",
    "    output_string = \"<script>$(\\\"div.input\\\").{}</script>\"\n",
    "    output_args   = (javascript_functions[state],)\n",
    "    output        = output_string.format(*output_args)\n",
    "\n",
    "    display(HTML(output))\n",
    "\n",
    "\n",
    "def button_action(value):\n",
    "\n",
    "    \"\"\"\n",
    "    Calls the toggle_code function and updates the button description.\n",
    "    \"\"\"\n",
    "\n",
    "    state = value.new\n",
    "\n",
    "    toggle_code(state)\n",
    "\n",
    "    value.owner.description = button_descriptions[state]\n",
    "\n",
    "\n",
    "state = False\n",
    "toggle_code(state)\n",
    "\n",
    "button = widgets.ToggleButton(state, description = button_descriptions[state])\n",
    "button.observe(button_action, \"value\")\n",
    "\n",
    "display(button)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte I"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las variables que pueden servir como predictores para entender si un hogar es pobre o no son:\n",
    "        -Variables que representan las características de la vivienda como el acceso al agua, a un baño, materiales de construcción y si está dentro de un barrio de emergencia. \n",
    "        -Variables que representan las características habitacionales del hogar como por ejemplo, la cantidad de cuartos destinados para dormir, régimen de tenecia del terreno y qué combustible se utiliza para cocinar. \n",
    "        -Variables denominada \"Estrategias del hogar\", entre ellas se encuentran si cobran un subsidio, si tienen una beca de estudio, si tuvieron que vender sus pertenencias en el último tiempo,  menores de edad trabajando, cantidad de personas que viven en el hogar, decil de ingresos, entre otras. Consideramos que esta última variable es muy relevante dado que marca la diferencia entre medir la pobreza a nivel hogar o a nivel individuo debido a que hogares pobres tienden a tener más miembros."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Abrimos la base de hogar \n",
    "df=pd.read_excel(\"usu_hogar_T122.xls\")\n",
    "df\n",
    "# Abrimos la tabla de usuarios \n",
    "df_usu=pd.read_excel(\"usu_individual_T122.xls\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CODUSU</th>\n",
       "      <th>ANO4</th>\n",
       "      <th>TRIMESTRE</th>\n",
       "      <th>NRO_HOGAR</th>\n",
       "      <th>REALIZADA</th>\n",
       "      <th>REGION</th>\n",
       "      <th>MAS_500</th>\n",
       "      <th>AGLOMERADO</th>\n",
       "      <th>PONDERA</th>\n",
       "      <th>IV1</th>\n",
       "      <th>...</th>\n",
       "      <th>GDECCFR</th>\n",
       "      <th>PDECCFR</th>\n",
       "      <th>ADECCFR</th>\n",
       "      <th>PONDIH</th>\n",
       "      <th>VII1_1</th>\n",
       "      <th>VII1_2</th>\n",
       "      <th>VII2_1</th>\n",
       "      <th>VII2_2</th>\n",
       "      <th>VII2_3</th>\n",
       "      <th>VII2_4</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>TQRMNOQSYHMOTOCDEIJAH00698520</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>S</td>\n",
       "      <td>33</td>\n",
       "      <td>4625</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>4410</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>TQRMNOPSSHKKMMCDEIIAD00780111</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>S</td>\n",
       "      <td>32</td>\n",
       "      <td>803</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>8</td>\n",
       "      <td>1440</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>TQRMNORSUHLMNPCDEIIAD00718267</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>S</td>\n",
       "      <td>32</td>\n",
       "      <td>2785</td>\n",
       "      <td>2</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>9</td>\n",
       "      <td>4073</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61</th>\n",
       "      <td>TQRMNORUPHMLRMCDEIJAH00698717</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>S</td>\n",
       "      <td>33</td>\n",
       "      <td>1698</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>TQRMNOTUYHMMKNCDEIJAH00698671</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>S</td>\n",
       "      <td>33</td>\n",
       "      <td>3764</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16778</th>\n",
       "      <td>TQRMNOPXQHJOKPCDEIJAH00718715</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>S</td>\n",
       "      <td>33</td>\n",
       "      <td>3278</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>5814</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16779</th>\n",
       "      <td>TQRMNOPXRHJOKPCDEIJAH00718716</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>S</td>\n",
       "      <td>33</td>\n",
       "      <td>3976</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>6427</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16780</th>\n",
       "      <td>TQRMNOPQSHMMKPCDEIJAH00780780</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>S</td>\n",
       "      <td>33</td>\n",
       "      <td>2607</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>4</td>\n",
       "      <td>5084</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16781</th>\n",
       "      <td>TQRMNOPWPHMLLLCDEIJAH00780781</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>S</td>\n",
       "      <td>33</td>\n",
       "      <td>2325</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>6</td>\n",
       "      <td>4528</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16782</th>\n",
       "      <td>TQRMNORUVHLLKQCDEIJAH00718720</td>\n",
       "      <td>2022</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>S</td>\n",
       "      <td>33</td>\n",
       "      <td>2102</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>12.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>12</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>98</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>2392 rows × 88 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                              CODUSU  ANO4  TRIMESTRE  NRO_HOGAR  REALIZADA  \\\n",
       "11     TQRMNOQSYHMOTOCDEIJAH00698520  2022          1          1          1   \n",
       "23     TQRMNOPSSHKKMMCDEIIAD00780111  2022          1          1          1   \n",
       "52     TQRMNORSUHLMNPCDEIIAD00718267  2022          1          1          1   \n",
       "61     TQRMNORUPHMLRMCDEIJAH00698717  2022          1          1          1   \n",
       "72     TQRMNOTUYHMMKNCDEIJAH00698671  2022          1          1          1   \n",
       "...                              ...   ...        ...        ...        ...   \n",
       "16778  TQRMNOPXQHJOKPCDEIJAH00718715  2022          1          1          1   \n",
       "16779  TQRMNOPXRHJOKPCDEIJAH00718716  2022          1          1          1   \n",
       "16780  TQRMNOPQSHMMKPCDEIJAH00780780  2022          1          1          1   \n",
       "16781  TQRMNOPWPHMLLLCDEIJAH00780781  2022          1          1          1   \n",
       "16782  TQRMNORUVHLLKQCDEIJAH00718720  2022          1          1          1   \n",
       "\n",
       "       REGION MAS_500  AGLOMERADO  PONDERA  IV1  ... GDECCFR  PDECCFR  \\\n",
       "11          1       S          33     4625    2  ...     0.0      NaN   \n",
       "23          1       S          32      803    1  ...    10.0      NaN   \n",
       "52          1       S          32     2785    2  ...    10.0      NaN   \n",
       "61          1       S          33     1698    1  ...    12.0      NaN   \n",
       "72          1       S          33     3764    1  ...    12.0      NaN   \n",
       "...       ...     ...         ...      ...  ...  ...     ...      ...   \n",
       "16778       1       S          33     3278    1  ...     1.0      NaN   \n",
       "16779       1       S          33     3976    1  ...     3.0      NaN   \n",
       "16780       1       S          33     2607    1  ...     3.0      NaN   \n",
       "16781       1       S          33     2325    1  ...     5.0      NaN   \n",
       "16782       1       S          33     2102    1  ...    12.0      NaN   \n",
       "\n",
       "       ADECCFR PONDIH  VII1_1  VII1_2  VII2_1  VII2_2 VII2_3  VII2_4  \n",
       "11           0   4410       1       0      98       0      0       0  \n",
       "23           8   1440       1       2      98       0      0       0  \n",
       "52           9   4073       2       0      98       0      0       0  \n",
       "61          12      0       2       1       3       0      0       0  \n",
       "72          12      0       2       0       1       0      0       0  \n",
       "...        ...    ...     ...     ...     ...     ...    ...     ...  \n",
       "16778        1   5814       2       0       3       4      0       0  \n",
       "16779        4   6427       2       1       3       4      0       0  \n",
       "16780        4   5084       1       2      98       0      0       0  \n",
       "16781        6   4528       1       2      98       0      0       0  \n",
       "16782       12      0       2       0      98       0      0       0  \n",
       "\n",
       "[2392 rows x 88 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Armamos un nuevo df solo con las observaciones de Buenos Aires y Gran Buenos Aires\n",
    "df_ba = df.loc[df['AGLOMERADO'].isin([32, 33])]\n",
    "df_ba\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CODUSU', 'ANO4_x', 'TRIMESTRE_x', 'NRO_HOGAR', 'REALIZADA', 'REGION_x', 'MAS_500_x', 'AGLOMERADO_x', 'PONDERA_x', 'IV1', 'IV1_ESP', 'IV2', 'IV3', 'IV3_ESP', 'IV4', 'IV5', 'IV6', 'IV7', 'IV7_ESP', 'IV8', 'IV9', 'IV10', 'IV11', 'IV12_1', 'IV12_2', 'IV12_3', 'II1', 'II2', 'II3', 'II3_1', 'II4_1', 'II4_2', 'II4_3', 'II5', 'II5_1', 'II6', 'II6_1', 'II7', 'II7_ESP', 'II8', 'II8_ESP', 'II9', 'V1', 'V2', 'V21', 'V22', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19_A', 'V19_B', 'IX_TOT', 'IX_MEN10', 'IX_MAYEQ10', 'ITF_x', 'DECIFR_x', 'IDECIFR_x', 'RDECIFR_x', 'GDECIFR_x', 'PDECIFR_x', 'ADECIFR_x', 'IPCF_x', 'DECCFR_x', 'IDECCFR_x', 'RDECCFR_x', 'GDECCFR_x', 'PDECCFR_x', 'ADECCFR_x', 'PONDIH_x', 'VII1_1', 'VII1_2', 'VII2_1', 'VII2_2', 'VII2_3', 'VII2_4', 'ANO4_y', 'TRIMESTRE_y', 'COMPONENTE', 'H15', 'REGION_y', 'MAS_500_y', 'AGLOMERADO_y', 'PONDERA_y', 'CH03', 'CH04', 'CH05', 'CH06', 'CH07', 'CH08', 'CH09', 'CH10', 'CH11', 'CH12', 'CH13', 'CH14', 'CH15', 'CH15_COD', 'CH16', 'CH16_COD', 'NIVEL_ED', 'ESTADO', 'CAT_OCUP', 'CAT_INAC', 'IMPUTA', 'PP02C1', 'PP02C2', 'PP02C3', 'PP02C4', 'PP02C5', 'PP02C6', 'PP02C7', 'PP02C8', 'PP02E', 'PP02H', 'PP02I', 'PP03C', 'PP03D', 'PP3E_TOT', 'PP3F_TOT', 'PP03G', 'PP03H', 'PP03I', 'PP03J', 'INTENSI', 'PP04A', 'PP04B_COD', 'PP04B1', 'PP04B2', 'PP04B3_MES', 'PP04B3_ANO', 'PP04B3_DIA', 'PP04C', 'PP04C99', 'PP04D_COD', 'PP04G', 'PP05B2_MES', 'PP05B2_ANO', 'PP05B2_DIA', 'PP05C_1', 'PP05C_2', 'PP05C_3', 'PP05E', 'PP05F', 'PP05H', 'PP06A', 'PP06C', 'PP06D', 'PP06E', 'PP06H', 'PP07A', 'PP07C', 'PP07D', 'PP07E', 'PP07F1', 'PP07F2', 'PP07F3', 'PP07F4', 'PP07F5', 'PP07G1', 'PP07G2', 'PP07G3', 'PP07G4', 'PP07G_59', 'PP07H', 'PP07I', 'PP07J', 'PP07K', 'PP08D1', 'PP08D4', 'PP08F1', 'PP08F2', 'PP08J1', 'PP08J2', 'PP08J3', 'PP09A', 'PP09A_ESP', 'PP09B', 'PP09C', 'PP09C_ESP', 'PP10A', 'PP10C', 'PP10D', 'PP10E', 'PP11A', 'PP11B_COD', 'PP11B1', 'PP11B2_MES', 'PP11B2_ANO', 'PP11B2_DIA', 'PP11C', 'PP11C99', 'PP11D_COD', 'PP11G_ANO', 'PP11G_MES', 'PP11G_DIA', 'PP11L', 'PP11L1', 'PP11M', 'PP11N', 'PP11O', 'PP11P', 'PP11Q', 'PP11R', 'PP11S', 'PP11T', 'P21', 'DECOCUR', 'IDECOCUR', 'RDECOCUR', 'GDECOCUR', 'PDECOCUR', 'ADECOCUR', 'PONDIIO', 'TOT_P12', 'P47T', 'DECINDR', 'IDECINDR', 'RDECINDR', 'GDECINDR', 'PDECINDR', 'ADECINDR', 'PONDII', 'V2_M', 'V3_M', 'V4_M', 'V5_M', 'V8_M', 'V9_M', 'V10_M', 'V11_M', 'V12_M', 'V18_M', 'V19_AM', 'V21_M', 'T_VI', 'ITF_y', 'DECIFR_y', 'IDECIFR_y', 'RDECIFR_y', 'GDECIFR_y', 'PDECIFR_y', 'ADECIFR_y', 'IPCF_y', 'DECCFR_y', 'IDECCFR_y', 'RDECCFR_y', 'GDECCFR_y', 'PDECCFR_y', 'ADECCFR_y', 'PONDIH_y']\n"
     ]
    }
   ],
   "source": [
    "df_ba_unido = pd.merge(df_ba, df_usu, on=(\"CODUSU\",\"NRO_HOGAR\"), how=\"inner\", validate=\"one_to_many\")\n",
    "df_ba_unido\n",
    "print(list(df_ba_unido.columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminamos todas las columnas duplicadas luego del merge, dado que se encontraban en ambas bases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['CODUSU', 'ANO4_x', 'TRIMESTRE_x', 'NRO_HOGAR', 'REALIZADA', 'REGION_x', 'MAS_500_x', 'AGLOMERADO_x', 'PONDERA_x', 'IV1', 'IV1_ESP', 'IV2', 'IV3', 'IV3_ESP', 'IV4', 'IV5', 'IV6', 'IV7', 'IV7_ESP', 'IV8', 'IV9', 'IV10', 'IV11', 'IV12_1', 'IV12_2', 'IV12_3', 'II1', 'II2', 'II3', 'II3_1', 'II4_1', 'II4_2', 'II4_3', 'II5', 'II5_1', 'II6', 'II6_1', 'II7', 'II7_ESP', 'II8', 'II8_ESP', 'II9', 'V1', 'V2', 'V21', 'V22', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10', 'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19_A', 'V19_B', 'IX_TOT', 'IX_MEN10', 'IX_MAYEQ10', 'ITF_x', 'DECIFR_x', 'IDECIFR_x', 'RDECIFR_x', 'GDECIFR_x', 'PDECIFR_x', 'ADECIFR_x', 'IPCF_x', 'DECCFR_x', 'IDECCFR_x', 'RDECCFR_x', 'GDECCFR_x', 'PDECCFR_x', 'ADECCFR_x', 'PONDIH_x', 'VII1_1', 'VII1_2', 'VII2_1', 'VII2_2', 'VII2_3', 'VII2_4', 'COMPONENTE', 'H15', 'CH03', 'CH04', 'CH05', 'CH06', 'CH07', 'CH08', 'CH09', 'CH10', 'CH11', 'CH12', 'CH13', 'CH14', 'CH15', 'CH15_COD', 'CH16', 'CH16_COD', 'NIVEL_ED', 'ESTADO', 'CAT_OCUP', 'CAT_INAC', 'IMPUTA', 'PP02C1', 'PP02C2', 'PP02C3', 'PP02C4', 'PP02C5', 'PP02C6', 'PP02C7', 'PP02C8', 'PP02E', 'PP02H', 'PP02I', 'PP03C', 'PP03D', 'PP3E_TOT', 'PP3F_TOT', 'PP03G', 'PP03H', 'PP03I', 'PP03J', 'INTENSI', 'PP04A', 'PP04B_COD', 'PP04B1', 'PP04B2', 'PP04B3_MES', 'PP04B3_ANO', 'PP04B3_DIA', 'PP04C', 'PP04C99', 'PP04D_COD', 'PP04G', 'PP05B2_MES', 'PP05B2_ANO', 'PP05B2_DIA', 'PP05C_1', 'PP05C_2', 'PP05C_3', 'PP05E', 'PP05F', 'PP05H', 'PP06A', 'PP06C', 'PP06D', 'PP06E', 'PP06H', 'PP07A', 'PP07C', 'PP07D', 'PP07E', 'PP07F1', 'PP07F2', 'PP07F3', 'PP07F4', 'PP07F5', 'PP07G1', 'PP07G2', 'PP07G3', 'PP07G4', 'PP07G_59', 'PP07H', 'PP07I', 'PP07J', 'PP07K', 'PP08D1', 'PP08D4', 'PP08F1', 'PP08F2', 'PP08J1', 'PP08J2', 'PP08J3', 'PP09A', 'PP09A_ESP', 'PP09B', 'PP09C', 'PP09C_ESP', 'PP10A', 'PP10C', 'PP10D', 'PP10E', 'PP11A', 'PP11B_COD', 'PP11B1', 'PP11B2_MES', 'PP11B2_ANO', 'PP11B2_DIA', 'PP11C', 'PP11C99', 'PP11D_COD', 'PP11G_ANO', 'PP11G_MES', 'PP11G_DIA', 'PP11L', 'PP11L1', 'PP11M', 'PP11N', 'PP11O', 'PP11P', 'PP11Q', 'PP11R', 'PP11S', 'PP11T', 'P21', 'DECOCUR', 'IDECOCUR', 'RDECOCUR', 'GDECOCUR', 'PDECOCUR', 'ADECOCUR', 'PONDIIO', 'TOT_P12', 'P47T', 'DECINDR', 'IDECINDR', 'RDECINDR', 'GDECINDR', 'PDECINDR', 'ADECINDR', 'PONDII', 'V2_M', 'V3_M', 'V4_M', 'V5_M', 'V8_M', 'V9_M', 'V10_M', 'V11_M', 'V12_M', 'V18_M', 'V19_AM', 'V21_M', 'T_VI']\n"
     ]
    }
   ],
   "source": [
    "# Observamos cuáles son las columnas que están repetidas, antes de eliminarlas:\n",
    "df_variables_dup = df_ba_unido.filter(regex='_y')\n",
    "df_variables_dup\n",
    "\n",
    "# Las eliminamos\n",
    "df_ba_unido.drop(columns=(df_ba_unido.filter(regex='_y')), inplace=True)\n",
    "\n",
    "print(list(df_ba_unido.columns))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizamos la función dropna del paquete de Pandas para eliminar columnas con observaciones vacias"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Limpiamos la base de los datos nulos\n",
    "# Eliminamos columnas que tienen más del 50% de observaciones vacías \n",
    "df_ba_unido_limpio = df_ba_unido.dropna(axis=1, thresh=3372)\n",
    "#df_ba_unido_limpio.to_excel(\"base2.xlsx\", sheet_name=\"sheet1\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chequeamos que ya no quedan missings values\n",
    "for i in range(len(df_ba_unido_limpio.index)) : \n",
    "    print(\"Número de missing values en la fila\", i + 1, \":\",\n",
    "         df_ba_unido_limpio.iloc[i].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos outliers \n",
    "for column in df_ba_unido_limpio.columns[1:]:\n",
    "    #print(df_ba_unido_limpio[column])\n",
    "    #print(df_ba_unido_limpio[column].dtype)\n",
    "    if df_ba_unido_limpio[column].dtype== \"str\":\n",
    "        continue\n",
    "    elif df_ba_unido_limpio[column].dtype== \"object\":\n",
    "        continue\n",
    "    else:\n",
    "        #print(df_ba_unido_limpio[column].dtype)\n",
    "        q_low = df_ba_unido_limpio[column].quantile(q = 0.01)\n",
    "        q_hi = df_ba_unido_limpio[column].quantile(q = 0.99)\n",
    "        f_filtrada = df_ba_unido_limpio[(df_ba_unido_limpio[column]<q_hi) & (df_ba_unido_limpio[column]>q_low)]\n",
    "f_filtrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for column in f_filtrada.columns[1:]:\n",
    "    if f_filtrada[column].dtype == \"int64\":\n",
    "        continue\n",
    "    elif f_filtrada[column].dtype == \"float64\":\n",
    "        continue\n",
    "    else:\n",
    "        print(column, \"es una variable\", f_filtrada[column].dtype)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Chequeamos cuál es el contenido de las variables que nos generarían problemas al no ser numéricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(list(f_filtrada.CH05))\n",
    "print(list(f_filtrada.MAS_500_x))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Observamos que CH05 contiene la fecha de nacimiento de los individuos (con lo cual podríamos borrarla, dado que tenemos su edad en la base también)\n",
    "\n",
    "MAS_500_x contienen una letra que indica el tamaño del aglomerado. Al habernos quedado con las observaciones de BsAs y Gran BsAs, todas tienen una \"S\".\n",
    "\n",
    "Debido a esto, consideramos que podemos deshacernos de tales variables sin mayores problemas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f_filtrada.drop(columns=\"CH05\", inplace=True)\n",
    "f_filtrada.drop(columns=\"MAS_500_x\", inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora, chequearemos que no queden variables de ingresos o edades con observaciones negativas en nuestra base de datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f_filtrada[f_filtrada[\"ITF_x\"] < 0])\n",
    "#Obtenemos un dataframe vacío, por lo que no hay observaciones que cumplan con esas características.\n",
    "\n",
    "print(f_filtrada[f_filtrada[\"IPCF_x\"] < 0])\n",
    "#Obtenemos un dataframe vacío, por lo que no hay observaciones que cumplan con esas características.\n",
    "\n",
    "print(f_filtrada[f_filtrada[\"P21\"] < 0])\n",
    "#Obtenemos un dataframe con datos de ingreso negativos, por lo que procedemos a eliminarlos.\n",
    "\n",
    "print(f_filtrada[f_filtrada[\"P47T\"] < 0])\n",
    "#Obtenemos un dataframe con datos de ingreso negativos, por lo que procedemos a eliminarlos.\n",
    "\n",
    "print(f_filtrada[f_filtrada[\"CH06\"] < 0])\n",
    "#Obtenemos un dataframe con datos de edad negativos, por lo que procedemos a eliminarlos."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Eliminamos variables con observaciones negativas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminamos las observaciones con edades negativas.\n",
    "f_filtrada = f_filtrada[f_filtrada.CH06>=0]\n",
    "\n",
    "#Eliminamos las observaciones con ingreso negativo.\n",
    "f_filtrada = f_filtrada[f_filtrada.P21>=0]\n",
    "\n",
    "#Eliminamos las observaciones con ingreso negativo.\n",
    "f_filtrada = f_filtrada[f_filtrada.P47T>=0]\n",
    "\n",
    "#Chequeamos que al imprimir el dataframe seleccionando edades negativas devuelva un dataframe vacío.\n",
    "print(f_filtrada[f_filtrada[\"CH06\"] < 0])\n",
    "\n",
    "#Chequeamos que al imprimir el dataframe seleccionando ingresos negativos devuelva un dataframe vacío.\n",
    "print(f_filtrada[f_filtrada[\"P21\"] < 0])\n",
    "\n",
    "#Chequeamos que al imprimir el dataframe seleccionando ingresos negativos devuelva un dataframe vacío.\n",
    "print(f_filtrada[f_filtrada[\"P47T\"] < 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Las variables que consideramos de interés para predecir la pobreza son: \n",
    "-V5_M: es el monto que recibe por subsidio o ayuda social\n",
    "-T_VI: monto total de ingresos no laborales\n",
    "-V11_M: monto de ingreso por beca de estudio\n",
    "-PP02I: variable binaria que indica si la persona trabajó en algún momento de los últimos 12 meses\n",
    "-IV12_3: variable binaria que indica si la vivienda está ubicada en un barrio de emergencia "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f_filtrada[\"V5_M\"].describe())\n",
    "print(f_filtrada[\"T_VI\"].describe())\n",
    "print(f_filtrada[\"V11_M\"].describe())\n",
    "print(f_filtrada[\"PP02I\"].describe())\n",
    "print(f_filtrada[\"IV12_3\"].describe())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tanto en el presente punto como en los dos siguientes recuperamos lo realizado en el TP2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Importamos la tabla de equivalencias \n",
    "tabla_adulto=pd.read_excel(\"tabla_adulto_equiv.xls\", header = 4, nrows=23, )\n",
    "tabla_adulto.rename({'Unnamed: 0':'Edad','Unnamed: 1':'Mujeres','Unnamed: 2':'Varones'}, axis=1, inplace = True)\n",
    "tabla_adulto\n",
    "# Generamos una nueva tabla para mujeres\n",
    "tabla_adulto_m = tabla_adulto[[\"Edad\",\"Mujeres\"]]\n",
    "tabla_adulto_m\n",
    "\n",
    "# Generamos una nueva tabla para varones\n",
    "tabla_adulto_v = tabla_adulto[[\"Edad\",\"Varones\"]]\n",
    "tabla_adulto_v\n",
    "# Generamos Ids para mujeres\n",
    "N=0 \n",
    "temp = 0\n",
    "lista = []\n",
    "for i in range(23):\n",
    "    temp = \"M\" + str(N)\n",
    "    lista.append(temp)\n",
    "    N=N+1\n",
    "\n",
    "print(lista)\n",
    "\n",
    "tabla_adulto_m[\"id\"]=lista\n",
    "tabla_adulto_m[\"Varon\"]=0\n",
    "tabla_adulto_m.rename({'Mujeres':'Valor'}, axis=1, inplace = True)\n",
    "tabla_adulto_m\n",
    "# Generamos Ids para varones\n",
    "N=0 \n",
    "temp_v = 0\n",
    "lista_v = []\n",
    "for i in range(23):\n",
    "    temp_v = \"V\" + str(N)\n",
    "    lista_v.append(temp_v)\n",
    "    N=N+1\n",
    "\n",
    "print(lista_v)\n",
    "\n",
    "tabla_adulto_v[\"id\"]=lista_v\n",
    "tabla_adulto_v[\"Varon\"]=1\n",
    "tabla_adulto_v.rename({'Varones':'Valor'}, axis=1, inplace = True)\n",
    "tabla_adulto_v\n",
    "\n",
    "# Unimos las tablas verticalmente\n",
    "tabla_adulto_total = tabla_adulto_v.append(tabla_adulto_m) \n",
    "tabla_adulto_total.reset_index(inplace=True, drop=True)\n",
    "tabla_adulto_total"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generamos un loop para asignar los códigos correspondientes (de sexo y edad) a cada observación de la base original\n",
    "for index, row in f_filtrada.iterrows(): \n",
    "    if row[\"CH06\"]<1:\n",
    "        if row[\"CH04\"]==1:\n",
    "            f_filtrada.loc[index,\"id\"]=\"V0\"\n",
    "        else: \n",
    "            f_filtrada.loc[index,\"id\"]=\"M0\"\n",
    "    elif row[\"CH06\"]==1:\n",
    "        if row[\"CH04\"]==1:\n",
    "            f_filtrada.loc[index,\"id\"]=\"V1\"\n",
    "        else: \n",
    "            f_filtrada.loc[index,\"id\"]=\"M1\"\n",
    "    elif row[\"CH06\"]==2:\n",
    "        if row[\"CH04\"]==1:\n",
    "            f_filtrada.loc[index,\"id\"]=\"V2\"\n",
    "        else: \n",
    "            f_filtrada.loc[index,\"id\"]=\"M2\"\n",
    "    elif row[\"CH06\"]==3:\n",
    "        if row[\"CH04\"]==1:\n",
    "            f_filtrada.loc[index,\"id\"]=\"V3\"\n",
    "        else: \n",
    "            f_filtrada.loc[index,\"id\"]=\"M3\"\n",
    "    elif row[\"CH06\"]==4:\n",
    "        if row[\"CH04\"]==1:\n",
    "            f_filtrada.loc[index,\"id\"]=\"V4\"\n",
    "        else: \n",
    "            f_filtrada.loc[index,\"id\"]=\"M4\"\n",
    "    elif row[\"CH06\"]==5:\n",
    "        if row[\"CH04\"]==1:\n",
    "            f_filtrada.loc[index,\"id\"]=\"V5\"\n",
    "        else: \n",
    "            f_filtrada.loc[index,\"id\"]=\"M5\"\n",
    "    elif row[\"CH06\"]==6:\n",
    "        if row[\"CH04\"]==1:\n",
    "            f_filtrada.loc[index,\"id\"]=\"V6\"\n",
    "        else: \n",
    "            f_filtrada.loc[index,\"id\"]=\"M6\"\n",
    "    elif row[\"CH06\"]==7:\n",
    "        if row[\"CH04\"]==1:\n",
    "            f_filtrada.loc[index,\"id\"]=\"V7\"\n",
    "        else: \n",
    "            f_filtrada.loc[index,\"id\"]=\"M7\"\n",
    "    elif row[\"CH06\"]==8:\n",
    "        if row[\"CH04\"]==1:\n",
    "            f_filtrada.loc[index,\"id\"]=\"V8\"\n",
    "        else: \n",
    "            f_filtrada.loc[index,\"id\"]=\"M8\"\n",
    "    elif row[\"CH06\"]==9:\n",
    "        if row[\"CH04\"]==1:\n",
    "            f_filtrada.loc[index,\"id\"]=\"V9\"\n",
    "        else: \n",
    "            f_filtrada.loc[index,\"id\"]=\"M9\"\n",
    "    elif row[\"CH06\"]==10:\n",
    "        if row[\"CH04\"]==1:\n",
    "            f_filtrada.loc[index,\"id\"]=\"V10\"\n",
    "        else: \n",
    "            f_filtrada.loc[index,\"id\"]=\"M10\"\n",
    "    elif row[\"CH06\"]==11:\n",
    "        if row[\"CH04\"]==1:\n",
    "            f_filtrada.loc[index,\"id\"]=\"V11\"\n",
    "        else: \n",
    "            f_filtrada.loc[index,\"id\"]=\"M11\"\n",
    "    elif row[\"CH06\"]==12:\n",
    "        if row[\"CH04\"]==1:\n",
    "            f_filtrada.loc[index,\"id\"]=\"V12\"\n",
    "        else: \n",
    "            f_filtrada.loc[index,\"id\"]=\"M12\"\n",
    "    elif row[\"CH06\"]==13:\n",
    "        if row[\"CH04\"]==1:\n",
    "            f_filtrada.loc[index,\"id\"]=\"V13\"\n",
    "        else: \n",
    "            f_filtrada.loc[index,\"id\"]=\"M13\"\n",
    "    elif row[\"CH06\"]==14:\n",
    "        if row[\"CH04\"]==1:\n",
    "            f_filtrada.loc[index,\"id\"]=\"V14\"\n",
    "        else: \n",
    "            f_filtrada.loc[index,\"id\"]=\"M14\"\n",
    "    elif row[\"CH06\"]==15:\n",
    "        if row[\"CH04\"]==1:\n",
    "            f_filtrada.loc[index,\"id\"]=\"V15\"\n",
    "        else:\n",
    "            f_filtrada.loc[index,\"id\"]=\"M15\"\n",
    "    elif row[\"CH06\"]==16:\n",
    "        if row[\"CH04\"]==1:\n",
    "            f_filtrada.loc[index,\"id\"]=\"V16\"\n",
    "        else: \n",
    "            f_filtrada.loc[index,\"id\"]=\"M16\"\n",
    "    elif row[\"CH06\"]==17:\n",
    "        if row[\"CH04\"]==1:\n",
    "            f_filtrada.loc[index,\"id\"]=\"V17\"\n",
    "        else: \n",
    "            f_filtrada.loc[index,\"id\"]=\"M17\"\n",
    "    elif (row[\"CH06\"]> 17 and row[\"CH06\"]<30):\n",
    "        if row[\"CH04\"]==1:\n",
    "            f_filtrada.loc[index,\"id\"]=\"V18\"\n",
    "        else: \n",
    "            f_filtrada.loc[index,\"id\"]=\"M18\"\n",
    "    elif (row[\"CH06\"]>= 30 and row[\"CH06\"]<46):\n",
    "        if row[\"CH04\"]==1:\n",
    "            f_filtrada.loc[index,\"id\"]=\"V19\"\n",
    "        else: \n",
    "            f_filtrada.loc[index,\"id\"]=\"M19\"\n",
    "    elif (row[\"CH06\"]>= 46 and row[\"CH06\"]<61):\n",
    "        if row[\"CH04\"]==1:\n",
    "            f_filtrada.loc[index,\"id\"]=\"V20\"\n",
    "        else: \n",
    "            f_filtrada.loc[index,\"id\"]=\"M20\"\n",
    "    elif (row[\"CH06\"]>= 61 and row[\"CH06\"]<75):\n",
    "        if row[\"CH04\"]==1:\n",
    "            f_filtrada.loc[index,\"id\"]=\"V21\"\n",
    "        else: \n",
    "            f_filtrada.loc[index,\"id\"]=\"M21\"\n",
    "    else:\n",
    "        if row[\"CH04\"]==1:\n",
    "            f_filtrada.loc[index,\"id\"]=\"V22\"\n",
    "        else: \n",
    "            f_filtrada.loc[index,\"id\"]=\"M22\"\n",
    "f_filtrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Chequeamos que se hayan asignado correctamente los id\n",
    "f_filtrada[[\"CH04\",\"CH06\",\"id\"]] \n",
    "# Unimos la tabla original con la tabla de equivalencias calóricas, a partir de los id generados \n",
    "df_ba_unido = pd.merge(f_filtrada, tabla_adulto_total, on=\"id\")\n",
    "df_ba_unido\n",
    "# Renombramos la variable de adulto_equiv \n",
    "df_ba_unido.rename({'Valor':'adulto_equiv'}, axis=1, inplace = True)\n",
    "df_ba_unido\n",
    "# Sumamos los valores calóricos por hogar y lo asignamos como una nueva variable a cada individuo\n",
    "\n",
    "df_ba_unido[\"ad_equiv_hogar\"] = df_ba_unido.groupby([\"CODUSU\", \"NRO_HOGAR\"])[\"adulto_equiv\"].transform('sum')\n",
    "df_ba_unido"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chequeamos la cantidad de personas que no declararon el ITF\n",
    "print(\"La cantidad de personas que no respondieron cuál es su ingreso total familiar es:\", df_ba_unido['ITF_x'].value_counts()[0])\n",
    "# Guardamos en un nuevo data frame las personas que sí respondieron a ITF\n",
    "respondieron = df_ba_unido[df_ba_unido[\"ITF_x\"]!=0]\n",
    "# Chequeamos que se haya guardado bien\n",
    "print(respondieron[\"ITF_x\"])\n",
    "# Guardamos en un nuevo data frame las personas que no respondieron a ITF\n",
    "norespondieron = df_ba_unido[df_ba_unido[\"ITF_x\"]==0]\n",
    "# Chequeamos que se haya guardado bien\n",
    "print(norespondieron[\"ITF_x\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Agregamos la columna a la base de los que respondieron\n",
    "respondieron[\"ingreso_necesario\"] = 27197.64 * respondieron[\"ad_equiv_hogar\"]\n",
    "#respondieron[[\"ingreso_necesario\" \"ad_equiv_hogar\"]]\n",
    "print(respondieron [\"ingreso_necesario\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos la columna que identifica si el individuo es pobre\n",
    "for index, row in respondieron.iterrows(): \n",
    "    if row[\"ITF_x\"]< respondieron.loc[index, \"ingreso_necesario\"]:\n",
    "        respondieron.loc [index,\"pobre\"] = 1\n",
    "    else:\n",
    "        respondieron.loc[index, \"pobre\"] = 0\n",
    "\n",
    "print(respondieron)\n",
    "\n",
    "# Contamos la cantidad de pobres\n",
    "\n",
    "print(\"En la base hay\", respondieron[\"pobre\"].value_counts()[1], \"personas pobres\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "respondieron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Concatenamos CODUSU y el número de hogar para poder quedarnos solo con una observación por hogar.\n",
    "for index, row in respondieron.iterrows(): \n",
    "    respondieron.loc[index, \"hogar\"] = str(respondieron.loc[index, \"CODUSU\"]) + str(respondieron.loc[index, \"NRO_HOGAR\"])    \n",
    "\n",
    "respondieron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Eliminamos las observaciones repetidas de un mismo hogar\n",
    "respondieron_hogares = respondieron.drop_duplicates(subset = \"hogar\", keep='first')\n",
    "respondieron_hogares"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sumamos los ponderadores de pobres y no pobres\n",
    "    \n",
    "pond_pob = 0\n",
    "\n",
    "for index, row in respondieron_hogares.iterrows(): \n",
    "    if respondieron_hogares.loc [index,\"pobre\"] == 1:\n",
    "        pond_pob += respondieron_hogares.loc [index,\"PONDIH_x\"]\n",
    "    else:\n",
    "        continue\n",
    "\n",
    "print(\"La suma del PONDIH de los hogares pobres es\", pond_pob)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Sumamos los ponderadores de todos los hogares\n",
    "pond_tot = respondieron_hogares[\"PONDIH_x\"].sum()\n",
    "print(\"La suma del PONDIH de todos los hogares\",pond_tot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculamos la tasa de pobreza por hogar\n",
    "\n",
    "print(\"La tasa de hogares bajo la linea de pobreza de AMBA es\", (pond_pob/pond_tot*100),\"%\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La tasa de pobreza que obtenemos se asemeja a la del INDEC. Para el primer semestre de 2022 para AMBA obtenemos una tasa de 25.36%, mientras que en el INDEC la tasa de pobreza para esta misma región es de 28.2%."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score \n",
    "from sklearn.metrics import roc_curve\n",
    "from sklearn.metrics import roc_auc_score\n",
    "from sklearn.metrics import RocCurveDisplay\n",
    "from sklearn.metrics import plot_roc_curve\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n",
    "from sklearn.discriminant_analysis import QuadraticDiscriminantAnalysis\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from scipy.special import expit\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def evalua_metodo(nombre_modelo, X_train, y_train, X_test, y_test, p, c) :\n",
    "    \n",
    "    '''\n",
    "    Esta función ajusta los datos al modelo provisto y calcula las siguientes métricas: matriz de confusión,\n",
    "    curva de ROC, valores AUC y el accuracy score del método. Automáticamente la función importa distintos paquetes de sklearn.\n",
    "    Input:\n",
    "        modelo (str): Modelo a utilizar. Las opciones son \"logit\", \"adl\" (análisis discriminante lineal), \"knn\" (vecinos\n",
    "        cercanos) \n",
    "        X_train (df): Partición de la muestra de predictores para entrenamiento\n",
    "        y_train (df): Partición de la muestra de outcomes para entrenamiento\n",
    "        X_test (df): Partición de la muestra de predictores para testeo\n",
    "        y_test (df): Partición de la muestra de outcomes para testeo\n",
    "    Output:\n",
    "        metricas (dict): Métricas de interés. Las métricas son matriz de confusión, curva de ROC, valores AUC, el accuracy\n",
    "        score del método y Error Cuadrático Medio.\n",
    "        \n",
    "    '''\n",
    "\n",
    "    metricas = {}\n",
    "    \n",
    "    MODELOS = {\"logit\": LogisticRegression(), \"adl\": LinearDiscriminantAnalysis(), \"knn\": KNeighborsClassifier()}\n",
    "    \n",
    "    modelo = MODELOS[nombre_modelo]\n",
    "    \n",
    "    if nombre_modelo == \"logit\":\n",
    "        # Ajustamos el clasificador con fit con la base de entrenamiento\n",
    "        log_reg = LogisticRegression(penalty = p, C= c, max_iter=10000, solver=\"liblinear\").fit(X_train, y_train)\n",
    "        # Predecimos con la base test\n",
    "        y_pred = log_reg.predict(X_test)\n",
    "        # Armamos la matriz de confusión.\n",
    "        matriz_confusion = confusion_matrix(y_test, y_pred)\n",
    "        \n",
    "        #group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "        #group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "        #        matriz_confusion.flatten()]\n",
    "        #group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "        #             matriz_confusion.flatten()/np.sum(matriz_confusion)]\n",
    "        #labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "        #  zip(group_names,group_counts,group_percentages)]\n",
    "        #labels = np.asarray(labels).reshape(2,2)\n",
    "        \n",
    "        # Hacemos la curva de ROC e imprimimos los valores de AUC\n",
    "        auc = roc_auc_score(y_test, y_pred)\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, y_pred)\n",
    "        # Graficamos la curva de ROC\n",
    "        #display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=auc, estimator_name='Reg_log')\n",
    "        #display.plot()  \n",
    "        #plt.plot([0, 1], [0, 1], color='red', linestyle='--')\n",
    "        #plt.show() \n",
    "        # Hacemos el accuracy score\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        k_usados = []\n",
    "        # Calculamos el ECM \n",
    "        ecm = mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    if nombre_modelo == \"adl\":\n",
    "        # Entrenamos el modelo para el análisis discriminante lineal\n",
    "        lda = LinearDiscriminantAnalysis()        \n",
    "        lda = lda.fit(X= X_train, y=y_train)\n",
    "        \n",
    "        # Realizamos las predicciones para la muestra de testeo\n",
    "        y_pred_lda = lda.predict(X_test)\n",
    "        # Armamos la matriz de confusión.\n",
    "        \n",
    "        matriz_confusion = confusion_matrix(y_test, y_pred_lda)\n",
    "        \n",
    "        #group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "        #group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "        #        matriz_confusion.flatten()]\n",
    "        #group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "        #             matriz_confusion.flatten()/np.sum(matriz_confusion)]\n",
    "        #labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "        #  zip(group_names,group_counts,group_percentages)]\n",
    "        #labels = np.asarray(labels).reshape(2,2)\n",
    "\n",
    "        \n",
    "        # Hacemos la curva de ROC e imprimimos los valores de AUC\n",
    "        auc = roc_auc_score(y_test, y_pred_lda)\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, y_pred_lda)\n",
    "        # Graficamos la curva de ROC\n",
    "        #display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=auc, estimator_name='Reg_lda')\n",
    "        #display.plot() \n",
    "        #plt.plot([0, 1], [0, 1], color='red', linestyle='--')\n",
    "        # Calculamos el accuracy score para análisis lineal discriminante\n",
    "        accuracy = accuracy_score(y_test, y_pred_lda)\n",
    "        k_usados = []\n",
    "        # Calculamos el ECM \n",
    "        ecm = mean_squared_error(y_test, y_pred_lda)\n",
    "\n",
    "    if nombre_modelo == \"knn\":\n",
    "        #Determinamos el k a utilizar\n",
    "        k_range = range(1,11)\n",
    "        scores = {}      \n",
    "        scores_list = []\n",
    "        for k in k_range:\n",
    "                knn = KNeighborsClassifier(n_neighbors=k)\n",
    "                knn.fit(X_train, y_train)\n",
    "                y_pred_knn = knn.predict(X_test)\n",
    "                scores[k] = accuracy_score(y_test, y_pred_knn)\n",
    "                scores_list.append(accuracy_score(y_test, y_pred_knn))\n",
    "        a_optimo = min(scores_list)\n",
    "        k_optimo = scores_list.index(a_optimo) + 1\n",
    "        # Entrenamos al modelo con el método de KNN, tomando el k optimo entre 1 y 10\n",
    "        knn = KNeighborsClassifier(n_neighbors= k_optimo)\n",
    "        knn.fit(X_train, y_train)\n",
    "        # Realizamos las predicciones para la muestra de testeo\n",
    "        y_pred_knn = knn.predict(X_test)\n",
    "        # Armamos la matriz de confusión.\n",
    "        \n",
    "        matriz_confusion = confusion_matrix(y_test, y_pred_knn)\n",
    "        \n",
    "        #group_names = ['True Neg','False Pos','False Neg','True Pos']\n",
    "        #group_counts = [\"{0:0.0f}\".format(value) for value in\n",
    "        #        matriz_confusion.flatten()]\n",
    "        #group_percentages = [\"{0:.2%}\".format(value) for value in\n",
    "        #             matriz_confusion.flatten()/np.sum(matriz_confusion)]\n",
    "        #labels = [f\"{v1}\\n{v2}\\n{v3}\" for v1, v2, v3 in\n",
    "        #  zip(group_names,group_counts,group_percentages)]\n",
    "        #labels = np.asarray(labels).reshape(2,2)\n",
    "        \n",
    "        # Hacemos la curva de ROC\n",
    "        auc = roc_auc_score(y_test, y_pred_knn)\n",
    "        fpr, tpr, thresholds = roc_curve(y_test, y_pred_knn)\n",
    "        # Graficamos la curva de ROC\n",
    "        #display = RocCurveDisplay(fpr=fpr, tpr=tpr, roc_auc=auc, estimator_name='Reg_knn')\n",
    "        #display.plot()  \n",
    "        #plt.plot([0, 1], [0, 1], color='red', linestyle='--')\n",
    "        # Calculamos el accuracy score\n",
    "        accuracy = accuracy_score(y_test, y_pred_knn)\n",
    "        k_usados = \"La cantidad de vecinos cercanos utilizados es \" + str(k_optimo)\n",
    "        # Calculamos el ECM \n",
    "        ecm = mean_squared_error(y_test, y_pred_knn)\n",
    "        \n",
    "\n",
    "    metricas = {\"Matriz de Confusion\": matriz_confusion, \"Valor AUC\" : \"%.4f\" %auc, \"Accuracy score\": \"%.2f\" %accuracy, \"ECM\": ecm}\n",
    "    \n",
    "    return metricas, k_usados"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Si bien inicialmente habíamos incluido la curva de ROC y una versión más completa de la matriz de confusión, estas nos traían problemas posteriormente por lo que Belén nos sugirió no graficarlas."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import KFold \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def cross_validation(nombre_modelo, k_particiones, X, y, p, c) :\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Esta función realiza validación cruzada con \"k_particiones\" iteraciones. Utiliza la función evalua_metodo en cada una de\n",
    "    las iteraciones y para cada una de las particiones. \n",
    "    Input:\n",
    "        modelo (str): Modelo a utilizar. Las opciones son \"logit\", \"adl\" (análisis discriminante lineal), \"knn\" (vecinos\n",
    "        cercanos) \n",
    "        k_particiones(int): Número entero que indica la cantidad de particiones a utilizar en la validación cruzada\n",
    "        X(df): Muestra de predictores\n",
    "        y(df): Muestra de outcomes\n",
    "    Output:\n",
    "        outputs (dict): Métricas de interés. El error Cuadrático Medio promedio de los modelos calculados\n",
    "        \n",
    "    '''\n",
    "        \n",
    "    K = k_particiones\n",
    "\n",
    "    ecms = pd.DataFrame(columns=[\"grado\", \"particion\", \"ECM\"])\n",
    "\n",
    "    for grado in range(2, 10):   \n",
    "\n",
    "        kf = KFold(n_splits=K, shuffle=True, random_state=100)\n",
    "\n",
    "        for i, (train_index, test_index) in enumerate(kf.split(X)):   \n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "            \n",
    "            sc = StandardScaler()\n",
    "\n",
    "            # Estandarizamos las observaciones de entrenamiento\n",
    "            X_train_transformed = pd.DataFrame(sc.fit_transform(X_train),index=X_train.index, columns=X_train.columns)\n",
    "\n",
    "            # Estandarizamos las observaciones de test\n",
    "            X_test_transformed = pd.DataFrame(sc.transform(X_test),index=X_test.index, columns=X_test.columns)\n",
    "            \n",
    "            X_train = X_train_transformed\n",
    "            X_test = X_test_transformed\n",
    "\n",
    "            metricas, k_usados = evalua_metodo(nombre_modelo, X_train, y_train, X_test, y_test, p, c)\n",
    "\n",
    "            ecms = ecms.append({\"grado\": grado, \"particion\": i, \"ECM\": metricas.get(\"ECM\")}, ignore_index=True)            \n",
    "\n",
    "    ecms = ecms.astype({\"grado\":int, \"particion\":int})\n",
    "    promedio_ecms = ecms[\"ECM\"].mean()\n",
    "    \n",
    "    return promedio_ecms\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evalua_config(configuraciones, nombre_modelo, k_particiones, X, y) :\n",
    "    \n",
    "    '''\n",
    "    Esta función recibe una lista de configuraciones de hiperparámetros y utilizando la función cross validation \n",
    "    obtine el error cuadratico medio para cada configuración y cual es la que genera menor error.\n",
    "    Input:\n",
    "        configuraciones (list): lista exahustiva de diccionarios de \n",
    "        posibles valores de configuraciones que incluye el hiperparametro.\n",
    "        modelo (str): Modelo a utilizar. Las opciones son \"logit\", \"adl\" (análisis discriminante lineal), \"knn\" (vecinos\n",
    "        cercanos) \n",
    "        k_particiones(int): Número entero que indica la cantidad de particiones a utilizar en la validación cruzada\n",
    "        X(df): Muestra de predictores\n",
    "        y(df): Muestra de outcomes\n",
    "        \n",
    "    Output:\n",
    "        optimos (dict): Métricas de interés. Esto es el error cuadratico medio óptimo y la configuracion correspondiente al \n",
    "        menor error cuadratico medio \n",
    "        \n",
    "    '''\n",
    "    ecm_optimo = np.inf\n",
    "    config_optimo = None\n",
    "    for config in configuraciones :\n",
    "        p = config[\"penalty\"]\n",
    "        c = config[\"C\"]\n",
    "        ecm_promedio = cross_validation(nombre_modelo, k_particiones, X, y, p, c)\n",
    "        if ecm_promedio < ecm_optimo :\n",
    "            ecm_optimo = ecm_promedio \n",
    "            config_optimo = config\n",
    "        \n",
    "    optimos = {\"Error cuadratico medio optimo\":ecm_optimo, \"Configuracion optima\": config_optimo}\n",
    "    return optimos, config_optimo, ecm_optimo\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def evalua_multiples_metodos(configuraciones, k_particiones, X, y):\n",
    "\n",
    "    '''\n",
    "    Esta función recibe una lista de configuraciones de hiperparámetros y utilizando la función cross validation \n",
    "    obtine el error cuadratico medio para cada configuración y cual es la que genera menor error.\n",
    "    Input:\n",
    "        configuraciones (list): lista exahustiva de diccionarios de posibles valores de configuraciones que incluye el hiperparametro.\n",
    "        k_particiones(int): Número entero que indica la cantidad de particiones a utilizar en la validación cruzada\n",
    "        X(df): Muestra de predictores\n",
    "        y(df): Muestra de outcomes\n",
    "        \n",
    "    Output:\n",
    "        df_modelos (df): Tabla con los diferentes modelos, sus configuraciones y métricas óptimas.\n",
    "        \n",
    "    '''\n",
    "\n",
    "    df_modelos = pd.DataFrame(columns=[\"Modelo\", \"Configuración\", \"Error Cuadratico Medio\", \"Matriz de confusion\", \"Valor AUC\", \"Accuracy score\", \"K usados en vecinos cercanos\"])\n",
    "\n",
    "    m = [\"logit\", \"adl\", \"knn\"]\n",
    "    \n",
    "    for l in m:\n",
    "        nombre_modelo = l\n",
    "        \n",
    "        if l == \"logit\": \n",
    "            optimos, config_optimo, ecm_optimo= evalua_config(configuraciones, \"logit\", k_particiones, X, y)\n",
    "            p = config_optimo[\"penalty\"]\n",
    "            c = config_optimo[\"C\"]\n",
    "            \n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 101)\n",
    "            \n",
    "            sc = StandardScaler()\n",
    "\n",
    "            # Estandarizamos las observaciones de entrenamiento\n",
    "            X_train_transformed = pd.DataFrame(sc.fit_transform(X_train),index=X_train.index, columns=X_train.columns)\n",
    "\n",
    "            # Estandarizamos las observaciones de test\n",
    "            X_test_transformed = pd.DataFrame(sc.transform(X_test),index=X_test.index, columns=X_test.columns)\n",
    "            \n",
    "            X_train = X_train_transformed\n",
    "            X_test = X_test_transformed             \n",
    "            \n",
    "            metricas, k_usados =  evalua_metodo(\"logit\", X_train, y_train, X_test, y_test, p, c)\n",
    "            df_modelos = df_modelos.append({\"Modelo\": \"Regresión Logística\", \"Configuración\": config_optimo, \"Error Cuadratico Medio\": ecm_optimo, \"Valor AUC\": metricas[\"Valor AUC\"], \"Accuracy score\": metricas[\"Accuracy score\"], \"K usados en vecinos cercanos\": \" \", \"Matriz de confusion\": metricas[\"Matriz de Confusion\"]}, ignore_index=True)\n",
    "        \n",
    "        elif l ==  \"adl\":\n",
    "            \n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 101)\n",
    "            \n",
    "            sc = StandardScaler()\n",
    "\n",
    "            # Estandarizamos las observaciones de entrenamiento\n",
    "            X_train_transformed = pd.DataFrame(sc.fit_transform(X_train),index=X_train.index, columns=X_train.columns)\n",
    "\n",
    "            # Estandarizamos las observaciones de test\n",
    "            X_test_transformed = pd.DataFrame(sc.transform(X_test),index=X_test.index, columns=X_test.columns)\n",
    "            \n",
    "            X_train = X_train_transformed\n",
    "            X_test = X_test_transformed  \n",
    "            \n",
    "            metricas, k_usados = evalua_metodo(\"adl\", X_train, y_train, X_test, y_test, p= None, c= None)\n",
    "            \n",
    "            df_modelos = df_modelos.append({\"Modelo\": \"Analisis discriminante lineal\", \"Configuración\": config_optimo, \"Error Cuadratico Medio\": metricas[\"ECM\"], \"Valor AUC\": metricas[\"Valor AUC\"], \"Accuracy score\": metricas[\"Accuracy score\"], \"K usados en vecinos cercanos\": \" \", \"Matriz de confusion\": metricas[\"Matriz de Confusion\"]}, ignore_index=True)\n",
    "        else:\n",
    "            X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 101)\n",
    "            \n",
    "            sc = StandardScaler()\n",
    "\n",
    "            # Estandarizamos las observaciones de entrenamiento\n",
    "            X_train_transformed = pd.DataFrame(sc.fit_transform(X_train),index=X_train.index, columns=X_train.columns)\n",
    "\n",
    "            # Estandarizamos las observaciones de test\n",
    "            X_test_transformed = pd.DataFrame(sc.transform(X_test),index=X_test.index, columns=X_test.columns)\n",
    "            \n",
    "            X_train = X_train_transformed\n",
    "            X_test = X_test_transformed \n",
    "            \n",
    "            metricas, k_usados = evalua_metodo(\"knn\", X_train, y_train, X_test, y_test, p= None, c= None)\n",
    "            df_modelos = df_modelos.append({\"Modelo\": \"Vecinos cercanos\", \"Configuración\": config_optimo, \"Error Cuadratico Medio\": metricas[\"ECM\"], \"Valor AUC\": metricas[\"Valor AUC\"], \"Accuracy score\": metricas[\"Accuracy score\"], \"K usados en vecinos cercanos\": k_usados, \"Matriz de confusion\": metricas[\"Matriz de Confusion\"]}, ignore_index=True)\n",
    "\n",
    "    return df_modelos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parte 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elimanamos todas las variables de ingreso que quedaban en el dataframe\n",
    "respondieron.drop(columns= [\"P21\", \"DECOCUR\", \"RDECOCUR\", \"GDECOCUR\", \"ADECOCUR\", \"PONDIIO\", \"TOT_P12\", \"P47T\", \"DECINDR\", \"RDECINDR\", \n",
    "\"GDECINDR\", \"ADECINDR\", \"PONDII\", \"V2_M\", \"V3_M\", \"V4_M\", \"V5_M\", \"V8_M\", \"V9_M\", \"V10_M\", \"V11_M\", \"V12_M\", \"V18_M\", \"V19_AM\", \"V21_M\", \"T_VI\", \"ITF_x\", \"DECIFR_x\",\n",
    "\"RDECIFR_x\", \"GDECIFR_x\", \"ADECIFR_x\", \"IPCF_x\", \"DECCFR_x\", \"RDECCFR_x\", \"GDECCFR_x\", \"ADECCFR_x\", \"PONDIH_x\"], inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos todas las variables de ingreso del dataframe \n",
    "norespondieron.drop(columns= [\"P21\", \"DECOCUR\", \"RDECOCUR\", \"GDECOCUR\", \"ADECOCUR\", \"PONDIIO\", \"TOT_P12\", \"P47T\", \"DECINDR\", \"RDECINDR\", \n",
    "\"GDECINDR\", \"ADECINDR\", \"PONDII\", \"V2_M\", \"V3_M\", \"V4_M\", \"V5_M\", \"V8_M\", \"V9_M\", \"V10_M\", \"V11_M\", \"V12_M\", \"V18_M\", \"V19_AM\", \"V21_M\", \"T_VI\", \"ITF_x\", \"DECIFR_x\",\n",
    "\"RDECIFR_x\", \"GDECIFR_x\", \"ADECIFR_x\", \"IPCF_x\", \"DECCFR_x\", \"RDECCFR_x\", \"GDECCFR_x\", \"ADECCFR_x\", \"PONDIH_x\"], inplace = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos las columnas adulto_equiv, ad_equiv_hogar y el id generado anteriormente por nuestra cuenta\n",
    "respondieron.drop(columns=[\"adulto_equiv\", \"ad_equiv_hogar\", \"ingreso_necesario\", \"id\"], inplace=True)\n",
    "norespondieron.drop(columns=[\"adulto_equiv\", \"ad_equiv_hogar\", \"id\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Eliminamos la columna de hogar por ser un tipo de id\n",
    "respondieron.drop(columns=[\"hogar\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chequeamos el dataframe con las variables restantes\n",
    "print(list(respondieron))\n",
    "print(list(norespondieron))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(respondieron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "respondieron.drop(columns=[\"Edad\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norespondieron.drop(columns=[\"Edad\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "respondieron.drop(columns=[\"CODUSU\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norespondieron.drop(columns=[\"CODUSU\"], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Establecemos el vector \"y\" y la matriz \"X\"\n",
    "y = respondieron[\"pobre\"]\n",
    "X = respondieron[respondieron.columns.difference([\"pobre\"])]\n",
    "# Añadimos la constante\n",
    "X[\"cte\"]=1\n",
    "X.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuraciones = [{\"penalty\":'l2', 'C':5}]\n",
    "df= evalua_multiples_metodos(configuraciones, 2, X, y)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para elegir el parámetro λ por Cross Validation, dividimos la muestra de entrenamiento en una determinada cantidad de particiones. A continuación, estimamos el modelo elegido con diferentes valores λ donde, para cada uno, lo estimamos tantas veces como particiones se determinen rotando la partición que se deja fuera para poder computar el ECM con la predicción realizada sobre esta. Así, para cada λ computamos el ECM promedio y elegimos la configuración que minimiza ECM promedio. Por otro lado, no elegimos el conjunto de test para su elección porque, en ese caso, estaríamos dándole a conocer al modelo los valroes de los datos que luego usamos para computar el ECM y así no podríamos chequear las predicciones de nuestro modelo inicial."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Según el valor de K que tomemos este será el tamaño que nos queda para las muestras de entrenamiento y de test.\n",
    "Si K es demasiado pequeño, tendremos una muestra de entrenamiento pequeña que nos dará probablemente un modelo sesgado.\n",
    "Si K es demasiado grande, nos quedará una muestra de test pequeña y además corremos el riesgo de tener un problema de overfit (un modelo que predice muy bien adentro de la muestra, pero mal fuera de ella). Por otro lado, si K = n estaremos estimando el modelo n veces con n - 1 datos, esto se conoce como el procedimiento \"leave one out\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Este es para probar que cross validation funcione\n",
    "configuraciones = [{\"penalty\":'l1', 'c':1/(10**-5)}]\n",
    "cross_validation(\"logit\", 2, X, y, \"l1\", 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Este es para probar que evalua_config funcione\n",
    "configuraciones = [{\"penalty\":'l1', 'C':1/(10**-5)}, {\"penalty\":'l1', 'C':1/(10**-4)}]\n",
    "   \n",
    "evalua_config(configuraciones, \"logit\", 2, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Elegimos el lamda óptimo con Lasso y Ridge"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Elegimos el lamda optimo con Lasso\n",
    "\n",
    "configuraciones = [{\"penalty\":'l1', 'C':1/(10**-5)}, {\"penalty\":'l1', 'C':1/(10**-4)}, {\"penalty\":'l1', 'C':1/(10**-3)}, {\"penalty\":'l1', 'C':1/(10**-2)}, {\"penalty\":'l1', 'C':1/(10**-1)}, {\"penalty\":'l1', 'C':1/(10**0)}, {\"penalty\":'l1', 'C':1/(10**1)}, {\"penalty\":'l1', 'C':1/(10**2)}, {\"penalty\":'l1', 'C':1/(10**3)}, {\"penalty\":'l1', 'C':1/(10**4)}, {\"penalty\":'l1', 'C':1/(10**5)}]\n",
    "\n",
    "   \n",
    "evalua_config(configuraciones, \"logit\", 10, X, y)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Elegimos el lamda optimo con Ridge\n",
    "\n",
    "configuraciones = [{\"penalty\":'l2', 'C':1/(10**-5)}, {\"penalty\":'l2', 'C':1/(10**-4)}, {\"penalty\":'l2', 'C':1/(10**-3)}, {\"penalty\":'l2', 'C':1/(10**-2)}, {\"penalty\":'l2', 'C':1/(10**-1)}, {\"penalty\":'l2', 'C':1/(10**0)}, {\"penalty\":'l2', 'C':1/(10**1)}, {\"penalty\":'l2', 'C':1/(10**2)}, {\"penalty\":'l2', 'C':1/(10**3)}, {\"penalty\":'l2', 'C':1/(10**4)}, {\"penalty\":'l2', 'C':1/(10**5)}]\n",
    "\n",
    "   \n",
    "evalua_config(configuraciones, \"logit\", 10, X, y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Tanto con Ridge como con Lasso el Lamda óptimo escogido es el lamda = 1. Sin embargo, es importante notar que en el caso de Lasso el ECM al utilizar este lamda es de 0.1980234, mientras que con Ridge, es de 0.199516."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Boxplots con distribución del error de predicción para cada Lamda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos una variación de la función de cross validation para poder graficar los boxplots\n",
    "%matplotlib inline\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import KFold \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def cross_validation_alt(nombre_modelo, k_particiones, X, y, configuracion) :\n",
    "    \n",
    "    \n",
    "    '''\n",
    "    Esta función realiza validación cruzada con \"k_particiones\" iteraciones. Utiliza la función evalua_metodo en cada una de\n",
    "    las iteraciones y para cada una de las particiones. \n",
    "    Input:\n",
    "        modelo (str): Modelo a utilizar. Las opciones son \"logit\", \"adl\" (análisis discriminante lineal), \"knn\" (vecinos\n",
    "        cercanos) \n",
    "        k_particiones(int): Número entero que indica la cantidad de particiones a utilizar en la validación cruzada\n",
    "        X(df): Muestra de predictores\n",
    "        y(df): Muestra de outcomes\n",
    "    Output:\n",
    "        outputs (dict): Métricas de interés. El error Cuadrático Medio promedio de los modelos calculados\n",
    "        \n",
    "    '''\n",
    "    p = configuracion[\"penalty\"]\n",
    "    c = configuracion[\"C\"]\n",
    "    \n",
    "    K = k_particiones\n",
    "\n",
    "    ecms = pd.DataFrame(columns=[\"grado\", \"particion\", \"ECM\"])\n",
    "\n",
    "    for grado in range(2, 10):   \n",
    "\n",
    "        kf = KFold(n_splits=K, shuffle=True, random_state=100)\n",
    "\n",
    "        for i, (train_index, test_index) in enumerate(kf.split(X)):   \n",
    "            X_train, X_test = X.iloc[train_index], X.iloc[test_index]\n",
    "            y_train, y_test = y.iloc[train_index], y.iloc[test_index]\n",
    "\n",
    "            sc = StandardScaler()\n",
    "\n",
    "            # Estandarizamos las observaciones de entrenamiento\n",
    "            X_train_transformed = pd.DataFrame(sc.fit_transform(X_train),index=X_train.index, columns=X_train.columns)\n",
    "\n",
    "            # Estandarizamos las observaciones de test\n",
    "            X_test_transformed = pd.DataFrame(sc.transform(X_test),index=X_test.index, columns=X_test.columns)\n",
    "            \n",
    "            X_train = X_train_transformed\n",
    "            X_test = X_test_transformed            \n",
    "            \n",
    "            metricas, k_usados = evalua_metodo(nombre_modelo, X_train, y_train, X_test, y_test, p, c)\n",
    "\n",
    "            ecms = ecms.append({\"grado\": grado, \"particion\": i, \"ECM\": metricas.get(\"ECM\")}, ignore_index=True)            \n",
    "\n",
    "    ecms = ecms.astype({\"grado\":int, \"particion\":int})\n",
    "    promedio_ecms = ecms[\"ECM\"].mean()\n",
    "    \n",
    "    return ecms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hacemos un loop para graficar los box plots para cada valor de lambda - LASSO\n",
    "import seaborn as sns\n",
    "\n",
    "configuraciones = [{\"penalty\":'l1', 'C':1/(10**-5)}, {\"penalty\":'l1', 'C':1/(10**-4)}, {\"penalty\":'l1', 'C':1/(10**-3)}, {\"penalty\":'l1', 'C':1/(10**-2)}, {\"penalty\":'l1', 'C':1/(10**-1)}, {\"penalty\":'l1', 'C':1/(10**0)}, {\"penalty\":'l1', 'C':1/(10**1)}, {\"penalty\":'l1', 'C':1/(10**2)}, {\"penalty\":'l1', 'C':1/(10**3)}, {\"penalty\":'l1', 'C':1/(10**4)}, {\"penalty\":'l1', 'C':1/(10**5)}]\n",
    "\n",
    "plt.figure(figsize=(15, 12))\n",
    "plt.subplots_adjust(hspace=0.5)\n",
    "plt.subplots_adjust(wspace=0.3)\n",
    "plt.suptitle(\"Boxplots para cada valor de lambda LASSO\", fontsize=18, y=0.95)\n",
    "\n",
    "for n, configuracion in enumerate(configuraciones):\n",
    "    ax = plt.subplot(3, 4, n + 1)\n",
    "    input = cross_validation_alt(\"logit\", 10, X, y, configuracion)\n",
    "    sns.set()\n",
    "    ss = sns.boxplot(data=input[\"ECM\"])\n",
    "    titulo = \"Boxplot para \" + str(configuracion)\n",
    "    ax.set_title(titulo)\n",
    "    \n",
    "plt.show()  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hacemos un loop para graficar los box plots para cada valor de lambda - RIDGE\n",
    "import seaborn as sns\n",
    "\n",
    "configuraciones = [{\"penalty\":'l2', 'C':1/(10**-5)}, {\"penalty\":'l2', 'C':1/(10**-4)}, {\"penalty\":'l2', 'C':1/(10**-3)}, {\"penalty\":'l2', 'C':1/(10**-2)}, {\"penalty\":'l2', 'C':1/(10**-1)}, {\"penalty\":'l2', 'C':1/(10**0)}, {\"penalty\":'l2', 'C':1/(10**1)}, {\"penalty\":'l2', 'C':1/(10**2)}, {\"penalty\":'l2', 'C':1/(10**3)}, {\"penalty\":'l2', 'C':1/(10**4)}, {\"penalty\":'l2', 'C':1/(10**5)}]\n",
    "\n",
    "fig = plt.figure(figsize=(15, 12))\n",
    "fig.subplots_adjust(hspace=0.5)\n",
    "fig.subplots_adjust(wspace=0.3)\n",
    "fig.suptitle(\"Boxplots para cada valor de lambda Ridge\", fontsize=18, y=0.95)\n",
    "fig.subplots(sharex=True)\n",
    "\n",
    "for n, configuracion in enumerate(configuraciones):\n",
    "    ax = plt.subplot(3, 4, n + 1)\n",
    "    input = cross_validation_alt(\"logit\", 10, X, y, configuracion)\n",
    "    sns.set()\n",
    "    ss = sns.boxplot(data=input[\"ECM\"])\n",
    "    titulo = \"Boxplot para \" + str(configuracion)\n",
    "    ax.set_title(titulo)\n",
    "\n",
    "fig.show()   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Boxplot con la proporción de variables ignoradas por el modelo en función de lamda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Hacemos un loop para graficar los box plots con la proporción de variables ignoradas por el modelo en función de lambda - Lasso\n",
    "import seaborn as sns\n",
    "\n",
    "configuraciones = [{\"penalty\":'l1', 'C':1/(10**-5)}, {\"penalty\":'l1', 'C':1/(10**-4)}, {\"penalty\":'l1', 'C':1/(10**-3)}, {\"penalty\":'l1', 'C':1/(10**-2)}, {\"penalty\":'l1', 'C':1/(10**-1)}, {\"penalty\":'l1', 'C':1/(10**0)}, {\"penalty\":'l1', 'C':1/(10**1)}, {\"penalty\":'l1', 'C':1/(10**2)}, {\"penalty\":'l1', 'C':1/(10**3)}, {\"penalty\":'l1', 'C':1/(10**4)}, {\"penalty\":'l1', 'C':1/(10**5)}]\n",
    "\n",
    "plt.title(\"Boxplots con la proporción de variables ignoradas por el modelo en función de lambda\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 101)\n",
    "\n",
    "input_bxp = []\n",
    "\n",
    "for n, configuracion in enumerate(configuraciones):    \n",
    "    log_reg = LogisticRegression(penalty = \"l1\", C = configuracion[\"C\"], max_iter=10000, solver=\"liblinear\")\n",
    "    log_reg.fit(X_train, y_train)\n",
    "    y_pred = log_reg.predict(X_test)\n",
    "\n",
    "    coef = log_reg.coef_\n",
    "    coef_reshape = coef.reshape((-1, 1))\n",
    "\n",
    "    coeficientes = pd.DataFrame(coef_reshape, index= X_train.columns)\n",
    "    coeficientes = coeficientes.reset_index()\n",
    "    coeficientes = coeficientes.set_axis(['var', 'coefic'], axis=1, inplace=False)\n",
    "    \n",
    "    ceros = []\n",
    "    for i, row in coeficientes.iterrows():\n",
    "        if coeficientes.loc[i,\"coefic\"] == 0:\n",
    "            ceros.append(coeficientes.loc[i, \"var\"])\n",
    "    \n",
    "    input_bxp.append(len(ceros))\n",
    "\n",
    "sns.set()\n",
    "ss = sns.boxplot(data=input_bxp)\n",
    "\n",
    "plt.show()   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Prueba para obtener los coeficientes del mejor modelo con uno de los lamdas\n",
    "\n",
    "configuracion = {\"penalty\":'l1', 'C':1/(10**-5)}\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 101)\n",
    "\n",
    "log_reg = LogisticRegression(penalty = \"l1\", C= 1/(10**-5), max_iter=10000, solver=\"liblinear\")\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "coef = log_reg.coef_\n",
    "coef_reshape = coef.reshape((-1, 1))\n",
    "\n",
    "coeficientes = pd.DataFrame(coef_reshape, index= X_train.columns)\n",
    "coeficientes = coeficientes.reset_index()\n",
    "coeficientes = coeficientes.set_axis(['var', 'coefic'], axis=1, inplace=False)\n",
    "\n",
    "coeficientes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ceros = []\n",
    "for i, row in coeficientes.iterrows():\n",
    "    if coeficientes.loc[i,\"coefic\"] == 0:\n",
    "        ceros.append(coeficientes.loc[i, \"var\"])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chequeamos qué variables fueron descartadas para el valor óptimo de LASSO del inciso anterior (lamda = 1)\n",
    "\n",
    "configuracion = {\"penalty\":'l1', 'C':1/(10**0)}\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 101)\n",
    "\n",
    "log_reg = LogisticRegression(penalty = \"l1\", C= 1/(10**0), max_iter=10000, solver=\"liblinear\", fit_intercept=False)\n",
    "log_reg.fit(X_train, y_train)\n",
    "y_pred = log_reg.predict(X_test)\n",
    "\n",
    "coef = log_reg.coef_\n",
    "coef_reshape = coef.reshape((-1, 1))\n",
    "\n",
    "print(\"Coeficientes del modelo con lamda óptimo:\")\n",
    "coeficientes_lamda_optimo = pd.DataFrame(coef_reshape, index= X_train.columns)\n",
    "\n",
    "coeficientes_lamda_optimo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exportamos la base para poder ver todos los coeficientes mejor:\n",
    "\n",
    "coeficientes_lamda_optimo.to_excel(\"Coeficientes_punto6.xlsx\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "En el caso del lamda óptimo hallado en el inciso anterior, las variables que son descartadas son:\n",
    "- CH16: Dónde vivía hace 5 años\n",
    "- H15: Entrevista individual realizada (sí/no)\n",
    "- II3: Utiliza alguna habitación del hogar exclusivamente como lugar de trabajo (consultorio, estudio, taller, negocio, etc.)\n",
    "- II4_1: Cuarto de cocina (sí/no)\n",
    "- II6: De los lugares planteados en la pregunta 4 utiliza alguno exclusivamente como lugar de trabajo\n",
    "- II6_1: No figura en el diccionario\n",
    "- IV11: Cómo es el desague del baño\n",
    "- IV12_1: La vivienda está ubicada cerca de un basural\n",
    "- IV12_3: La vivienda está ubicada en villa de emergencia\n",
    "- IX_MEN10: Cantidad de miembros del hogar menores de 10 años\n",
    "- PP02C1: Hizo contactos, entrevistas (búsqueda laboral)\n",
    "- PP02C3: Se presentó en establecimientos (búsqueda laboral)\n",
    "- PP02C4: Hizo algo para ponerse por su cuenta (búsqueda laboral)\n",
    "- PP02C5: Puso carteles en negocios, preguntó en el barrio (búsqueda laboral)\n",
    "- PP02C8: De otra forma activa (búsqueda laboral)\n",
    "- PP02H: En los últimos 12 meses, ¿buscó trabajo en algún momento?\n",
    "- REALIZADA: Entrevista realizada (hogar respondió o no)\n",
    "- REGION_x\n",
    "- TRIMESTRE_x\n",
    "- V15: En los últimos tres meses, las personas del hogar han vivido pedir préstamos a bancos, financieras, etc.\n",
    "- V18: Tuvieron otros ingresos en efectivo (limosnas, juegos de azar, etc.)\n",
    "- V19_A: ¿Menores de 10 años ayudan con algún dinero trabajando?\n",
    "- V19_B: ¿Menores de 10 años ayudan con algún menores pidiendo?\n",
    "- V3: En los últimos tres meses, las personas del hogar han vivido de indemnización por despido\n",
    "- V4: En los últimos tres meses, las personas del hogar han vivido de seguro de desempleo\n",
    "- V9: En los últimos tres meses, las personas del hogar han vivido ganancias de algún negocio en el que no trabajan\n",
    "- VII2_4: Otras personas que ayudan en las tareas de la casa\n",
    "- Varon: Variable armada por nosotras para poder adjuntar los valores de la tabla de equivalencias\n",
    "- cte\n",
    "\n",
    "En general, podemos observar que todas las variables que habíamos considerado como relevantes para evaluar los niveles de pobreza han sido conservadas por el modelo. Las que se han descartado tienen que ver con detalles más específicos acerca de las viviendas o los individuos, que no habíamos considerado como relevantes. Sin embargo, hay cuatro variables que nos llama la atención que no hayan sumado poder predictivo al modelo y hayan sido descartadas como, por ejemplo, si los menores de edad trabajan o ayudan pidiendo, si el hogar está cerca de un basural o si el hogar está en en un barrio de emergencia."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para responder este punto nos referiremos a todos los modelos de regresión logística que evaluamos en el inciso 5 de esta parte III del trabajo. En particular, aprovecharemos el hecho de que ambos métodos de regularización (Ridge y Lasso) obtuvieron el mismo lamda (lamda=1) como hiperparámetro óptimo. En este sentido, si observamos el ECM producido por cada una de los modelos de predicción, veremos que Lasso es el método de regularización que mejor funcionó: su ECM fue de 0.1980234, mientras que el de Ridge, fue de 0.199516."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "configuraciones = [{\"penalty\":'l1', 'C':1},{\"penalty\":'l2', 'C':1}]\n",
    "df= evalua_multiples_metodos(configuraciones, 10, X, y)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para determinar cuál de todos los métodos es el que predice mejor, recurrimos a la función \"evalua_multiples_metodos\". Mediante esta, evaluamos diferentes métodos de predicción, tomando como configuración los hiperparámetros que en los incisos anteriores habíamos encontrado como los óptimos para el modelo de regresión logística. \n",
    "\n",
    "Así, podemos observar que tanto el modelo de \"Regresión Logística\" (con Lamda igual a 1 y el método de Lasso) como el de \"Análisis Discrimante Lineal\" están muy cercanos con los valores de las medidas de precisión. Incluso, se puede señalar que coinciden en el valor del \"Accuracy score\". Sin embargo, si nos guiáramos estrictamente por la medida del ECM, el modelo que convendría elegir dado que es el que mejor predice es el de Análisis Discriminante Lineal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "norespondieron[\"cte\"] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lda = LinearDiscriminantAnalysis()        \n",
    "lda = lda.fit(X_train, y_train)\n",
    "        \n",
    "# Realizamos las predicciones para la muestra de testeo\n",
    "pobres_pred_lda = lda.predict(norespondieron)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Hogares pobres predichos:\", np.sum(pobres_pred_lda))\n",
    "print(\"Hogares que no reportaron ingreso:\", len(norespondieron))\n",
    "print(\"Proporción de hogares pobres:\", (np.sum(pobres_pred_lda))/(len(norespondieron))*100, \"%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vemos que la proporción de hogares pobres obtenida de nuestra predicción con análisis discriminante lineal es del 100%. Nos resulta extraño por lo que creemos que puede haber algún error previo en la estimación. Por lo tanto chequeamos el modelo con las x de entrenamiento (sabemos que está bien porque el modelo se construyó en base a eso), lo que nos da una proporción de pobres del 42,47% , más similar a la declarada por el INDEC."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "Tutorial 8.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
