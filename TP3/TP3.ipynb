{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.6"},"colab":{"name":"Tutorial 8.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"_m3CnbNMV2Cp"},"source":["# Tutorial de Big Data\n","## Bienvenidos a la clase 8 \n","\n","**Objetivo:** \n","Que se familiaricen con la técnica de K-fold Cross Validation\n","\n","### Temario:\n","- Preguntas de GitHub: Recuerden que todos los miembros del equipo tienen que hacer al menos un push para el TP3\n","- Practica para no repetir código\n","- K-fold cross validation\n"]},{"cell_type":"code","metadata":{"id":"BoSQOK1iV2Cs"},"source":["import pandas as pd\n","import numpy as np\n","\n","from matplotlib import pyplot as plt\n","\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LinearRegression\n","from sklearn.preprocessing import PolynomialFeatures \n","from sklearn.metrics import mean_squared_error"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"_fRHNPS6V2Ct"},"source":["auto = pd.read_csv(\"Auto.csv\")\n","\n","# Ver datos sobre la base en [link](https://rdrr.io/cran/ISLR/man/Auto.html)\n","\n","# Guardo los vectores de variable dependiente y de variable independiente respectivamente:\n","y = auto['mpg']\n","X = auto['horsepower']\n","X = np.array(X).reshape((-1, 1))\n","\n","# Parto la base en dos y transformo el vector x: \n","x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 101)\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"roKZmBmrV2Cu"},"source":["# Recuerdan que hemos visto Regresiones Polinomicas, las cuales implican una \n","# transformación polinomica de las X, para luego implementar la regresión?\n","\n","# Veamos un modelo cuadrático:\n","poly = PolynomialFeatures(degree = 2, include_bias=False) \n","\n","# Recordemos en esta instancia setear include_bias a False dado que en la \n","# regresión lineal se incluirá la columna de 1s.\n","\n","#print(x_train)\n","x_train_poly = poly.fit_transform(x_train)\n","x_test_poly = poly.fit_transform(x_test)  \n","  \n","#print(x_train_poly)\n","\n","model = LinearRegression().fit(x_train_poly, y_train) \n","\n","print('Intercepto:', model.intercept_)\n","print('Pendiente:', model.coef_)\n","\n","# Calculamos el Error Cuadrático Medio\n","y_predpoly = model.predict(x_test_poly)\n","ecm2 = mean_squared_error(y_predpoly, y_test)\n","ecm2"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"PGeVHCwAV2Cv"},"source":["# Veamos un modelo cubico:\n","poly = PolynomialFeatures(degree = 3, include_bias=False) \n","\n","x_train_poly = poly.fit_transform(x_train)\n","x_test_poly = poly.fit_transform(x_test)  \n","  \n","model = LinearRegression().fit(x_train_poly, y_train) \n","y_predpoly = model.predict(x_test_poly)\n","\n","ecm3 = mean_squared_error(y_predpoly, y_test)\n","ecm3"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"g0v8qlN3V2Cv"},"source":["X_seq = np.linspace(X.min(), X.max()).reshape(-1,1)\n","X_seq_poly = poly.fit_transform(X_seq)  \n","\n","plt.figure()\n","plt.scatter(x_train, y_train)\n","plt.plot(X_seq, model.predict(X_seq_poly),color=\"black\")\n","plt.title(\"Polynomial regression with degree 3\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"wi5RpHRPV2Cw"},"source":["X_seq = np.linspace(X.min(), X.max()).reshape(-1,1)\n","X_seq_poly = poly.fit_transform(X_seq)  \n","\n","plt.figure()\n","plt.scatter(x_test, y_test)\n","plt.plot(X_seq, model.predict(X_seq_poly),color=\"black\")\n","plt.title(\"Polynomial regression with degree 3\")\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"-zMPx8mCV2Cx"},"source":["# Ahora supongamos que cambio la muestra y hago todo lo mismo.\n","x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 500)\n","\n","# Que error esperarian que obtengamos esta vez?\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Kvzk8UGzV2Cx"},"source":["# Como podemos repetir el código sin escribirlo por tercera vez?\n","# Podemos hacer que nuestro código funcione para otros grados?\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ZbegRYh6V2Cy"},"source":["ecm2b = transformacion_y_regresion(2, x_train, x_test, y_train, y_test)\n","ecm3b = transformacion_y_regresion(3, x_train, x_test, y_train, y_test)\n","ecm4 = transformacion_y_regresion(4, x_train, x_test, y_train, y_test)\n","ecm5 = transformacion_y_regresion(5, x_train, x_test, y_train, y_test)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"9BjUojF5V2Cy"},"source":["print('Grado2:', ecm2)\n","print('Grado3:', ecm3)\n","\n","print('\\nGrado2:', ecm2b)\n","print('Grado3:', ecm3b)\n","print('Grado4:', ecm4)\n","print('Grado5:', ecm5)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"UDGNMpRXV2Cz"},"source":["# Otra forma de hacer lo mismo pero aprovechando el metodo pipeline:\n","from sklearn.pipeline import make_pipeline\n","\n","grado = 3\n","polyreg = make_pipeline(PolynomialFeatures(grado),LinearRegression())\n","modelo = polyreg.fit(x_train, y_train)\n","y_pred_poly = modelo.predict(x_test)\n","\n","    \n","# Calculamos el Error Cuadrático Medio\n","ecm = mean_squared_error(y_pred_poly, y_test)\n","print(ecm)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"hk05DUbrV2Cz"},"source":["###  K-FOLD CROSS-VALIDATION  \n"]},{"cell_type":"code","metadata":{"id":"ktqljY-fV2Cz"},"source":["y = auto['mpg']\n","X = auto['horsepower']\n","X = np.array(X).reshape((-1, 1))\n","\n","from sklearn.model_selection import KFold\n","\n","K = 5\n","\n","ecms = pd.DataFrame(columns=[\"grado\", \"particion\", \"ecm\"])\n","\n","for grado in range(2, 10):   \n","\n","    kf = KFold(n_splits=K, shuffle=True, random_state=100)\n","    \n","    # El método kf.split aplicado a X nos da los conjuntos de índices que necesitamos para\n","    # partir nuestros conjunto de datos en training y testing en cada iteración.\n","    #  OXXXX\n","    #  XOXXX\n","    #  XXOXX\n","    #  XXXOX\n","    #  XXXXO\n","    \n","    for i, (train_index, test_index) in enumerate(kf.split(X)):   \n","        x_train, x_test = X[train_index], X[test_index]\n","        y_train, y_test = y[train_index], y[test_index]\n","        \n","        ecm, modelo = transformacion_y_regresion(grado, x_train, x_test, y_train, y_test)\n","            \n","        ecms = ecms.append({\"grado\": grado, \"particion\": i, \"ecm\": ecm}, ignore_index=True)            \n","    \n","ecms = ecms.astype({\"grado\":int, \"particion\":int})\n","ecms"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ThaTPH93V2C0"},"source":["# Otra opción podría ser visualizandolo en un boxplot\n","import seaborn as sns\n","sns.set()\n","ss = sns.boxplot(data=mses, x=\"grado\", y=\"mse\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"BJJix54xV2C0"},"source":["#Una opción para ver el mejor modelo sería sacar el error promedio para cada grado:\n","ecms.groupby('grado').agg({'ecm':'mean'})"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"lr5xl6PxV2C0"},"source":["#Construyan una función que les permita seleccionar \n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"O14qkQTSV2C0"},"source":["#Fianalmente construimos el modelo polinomial de grado 6 y lo graficamos \n","x_train, x_test, y_train, y_test = train_test_split(X, y, test_size = 0.3, random_state = 101)\n","\n","ecm, modelo = transformacion_y_regresion(grado, x_train, x_test, y_train, y_test)\n","        \n","X_seq = np.linspace(X.min(), X.max()).reshape(-1,1)\n","poly = PolynomialFeatures(degree = grado, include_bias=False) \n","X_seq_poly = poly.fit_transform(X_seq)  \n","\n","plt.figure()\n","plt.scatter(x_train, y_train)\n","plt.plot(X_seq, modelo.predict(X_seq_poly),color=\"black\")\n","plt.title(\"Polynomial regression with degree {}\".format(grado))\n","plt.show()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"mCSNWhSnV2C0"},"source":[""],"execution_count":null,"outputs":[]}]}